# 主成分分析（PCA）

主成分分析（PCA）是一种常用的降维技术，它通过找出数据中最大方差的方向，将数据投影到这些方向上，从而减少数据的维度。PCA在许多领域都有广泛的应用，包括图像处理、模式识别和数据压缩等。

## PCA的基本步骤

### 1. 标准化数据
首先，将数据标准化，即对每个变量减去均值并除以标准差，使得每个变量都有零均值和单位方差。这一步保证了不同尺度的变量在PCA中具有相同的影响。

![image](https://github.com/BoBo1529707515/EEG-analyse/assets/145309276/0b7d6b98-e2ab-4585-aa13-f2c9f4fc7eaa)


其中，$x_{ij}$ 是第 i 个样本的第 j个变量值，![image](https://github.com/BoBo1529707515/EEG-analyse/assets/145309276/8d3762b7-61f6-464f-b4ca-a964d98d2e1e)
 是第 $j$ 个变量的均值，s_j 是第 j 个变量的标准差。  
**为什么要除以标准差**  
平衡变量的影响
假设我们有一个数据集，其中包含两个变量：身高（以米为单位）和体重（以公斤为单位）。原始数据可能如下：  
![image](https://github.com/BoBo1529707515/EEG-analyse/assets/145309276/f445b423-fd0a-4027-b27f-efda474b4050)  
原始数据的问题：
在这个数据集中，身高的数据范围大约在1.6到1.8之间，而体重的数据范围在50到70之间。因为体重的数值范围更大，如果不进行标准化，体重对PCA的影响会远远大于身高。

### 2. 计算协方差矩阵
计算标准化后的数据的协方差矩阵。协方差矩阵表示变量之间的线性关系。

$$
\Sigma = \frac{1}{N-1} \sum_{i=1}^{N} (x'_i - \bar{x}')(x'_i - \bar{x}')^T
$$

其中，$x'_i$ 是第 $i$ 个样本的标准化数据向量，![image](https://github.com/BoBo1529707515/EEG-analyse/assets/145309276/dedc9145-f068-4741-93c3-9058ccd5a908)
 是标准化数据的均值向量。

### 3. 计算特征值和特征向量
对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。特征向量表示数据中主要的变异方向，特征值表示这些方向上的变异程度。

$$
\Sigma \mathbf{v}_i = \lambda_i \mathbf{v}_i
$$

其中，λi是第 $i$ 个特征值，vi是对应的特征向量。

### 4. 选择主成分
根据特征值的大小选择前 $k$ 个主成分。这些主成分对应的特征向量组成了新的坐标系，数据将在这个新的坐标系中表示。

### 5. 转换数据
将原始数据投影到选定的主成分上，得到降维后的数据。

$$
Z = X \mathbf{V}_k
$$

其中，$X$ 是原始数据矩阵，Vk 是前 $k$ 个特征向量组成的矩阵，$Z$ 是降维后的数据。


  ## 例子


假设我们有一个包含 2 个变量的数据集 $X$：

$$
X = \begin{pmatrix}
2.5 & 2.4 \\
0.5 & 0.7 \\
2.2 & 2.9 \\
1.9 & 2.2 \\
3.1 & 3.0 \\
2.3 & 2.7 \\
2.0 & 1.6 \\
1.0 & 1.1 \\
1.5 & 1.6 \\
1.1 & 0.9
\end{pmatrix}
$$

### 步骤 1：标准化数据

计算每个变量的均值和标准差，然后标准化数据：

$$
\text{均值} = \begin{pmatrix}
1.81 \\
1.91
\end{pmatrix}, \quad
\text{标准差} = \begin{pmatrix}
0.83 \\
0.92
\end{pmatrix}
$$

标准化后的数据矩阵 $X'$ 为：

$$
X' = \begin{pmatrix}
0.82 & 0.54 \\
-1.57 & -1.30 \\
0.46 & 1.08 \\
0.11 & 0.32 \\
1.56 & 1.18 \\
0.59 & 0.86 \\
0.23 & -0.65 \\
-0.98 & -0.87 \\
-0.37 & -0.34 \\
-0.85 & -1.10
\end{pmatrix}
$$

### 步骤 2：计算协方差矩阵

计算标准化数据的协方差矩阵：

$$
\Sigma = \begin{pmatrix}
1.00 & 0.94 \\
0.94 & 1.00
\end{pmatrix}
$$

### 步骤 3：计算特征值和特征向量

对协方差矩阵进行特征值分解，得到特征值和特征向量：

$$
\lambda_1 = 1.94, \quad \lambda_2 = 0.06
$$

![image](https://github.com/BoBo1529707515/EEG-analyse/assets/145309276/33853447-bad3-46de-86c7-4265d356ff54)



对应的特征向量为：

$$
\mathbf{v}_1 = \begin{pmatrix}
0.71 \\
0.71
\end{pmatrix}, \quad \mathbf{v}_2 = \begin{pmatrix}
-0.71 \\
0.71
\end{pmatrix}
$$

### 步骤 4：选择主成分

选择特征值最大的前一个特征向量作为主成分。

### 步骤 5：转换数据

将原始数据投影到主成分上，得到降维后的数据：

$$
Z = \begin{pmatrix}
0.9656 \\
-2.0377 \\
1.0934 \\
0.3053 \\
1.9414 \\
1.0275 \\
-0.2972 \\
-1.3167 \\
-0.5037 \\
-1.3779
\end{pmatrix}
$$



## 总结

PCA通过以下步骤来实现降维：

1. 标准化数据。
2. 计算协方差矩阵。
3. 计算协方差矩阵的特征值和特征向量。
4. 选择主成分。
5. 将原始数据投影到主成分上。

通过这些步骤，PCA能够有效地减少数据维度，同时保留尽可能多的信息，使得数据分析和处理更加高效。
# 2.进阶理解  
PCA的本质就是旋转维度，使得投影在某一方向上的离散数据方差最大**将数据整体旋转后，根据数据在各维度上投影的方差值由小到大地舍弃维度。**
